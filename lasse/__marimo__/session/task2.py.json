{
  "version": "1",
  "metadata": {
    "marimo_version": "0.18.4"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "53f9041862d9ae651859a3535160fbdd",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "97f9cdb08279be57cfa6560f107247e5",
      "outputs": [
        {
          "type": "data",
          "data": {
            "application/json": "[[5625, 46], [5625], [1407, 46], [1407]]"
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "/home/lasse/Documents/aml_exam/src/data.py:34: ParserWarning: Both a converter and dtype were specified for column Churn - only the converter will be used.\n  data = pd.read_csv(\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "vblA",
      "code_hash": "6b17f5c0c967c6e67ff98dceb85fc1e9",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "2025/12/28 11:39:47 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n2025/12/28 11:39:47 INFO mlflow.store.db.utils: Updating database tables\n2025/12/28 11:39:47 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2025/12/28 11:39:47 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2025/12/28 11:39:47 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n2025/12/28 11:39:47 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n2025/12/28 11:39:48 INFO mlflow.tracking.fluent: Experiment with name 'AML Task 2' does not exist. Creating a new experiment.\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "bkHC",
      "code_hash": "e99feef9af0c08ba060d9380ffa1a184",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stderr",
          "text": "/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:490: FitFailedWarning: \n5 fits failed out of a total of 50.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1336, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1147, in fit\n    solver = _check_solver(self.solver, penalty, self.dual)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 65, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1137: UserWarning: One or more of the test scores are non-finite: [0.80195556 0.8016     0.80142222 0.80106667        nan 0.80035556\n 0.80177778 0.80177778 0.8016     0.79271111]\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.7981520966595593\nTest ROC AUC: 0.848780487804878\n              precision    recall  f1-score   support\n\n           0       0.85      0.88      0.86      1025\n           1       0.65      0.57      0.60       382\n\n    accuracy                           0.80      1407\n   macro avg       0.75      0.73      0.73      1407\nweighted avg       0.79      0.80      0.79      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "lEQa",
      "code_hash": "83cd206b10944df4cdd6031381f2c368",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Fitting 3 folds for each of 50 candidates, totalling 150 fits\nROC AUC: 0.8427403907546929\nAccuracy: 0.7960199004975125\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.87      1025\n           1       0.66      0.52      0.58       382\n\n    accuracy                           0.80      1407\n   macro avg       0.75      0.71      0.72      1407\nweighted avg       0.79      0.80      0.79      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "PKri",
      "code_hash": "24875fa9615e4faa0a6458b64531383c",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Fitting 3 folds for each of 50 candidates, totalling 150 fits\nROC AUC: 0.8459187843187334\nAccuracy: 0.798862828713575\n              precision    recall  f1-score   support\n\n           0       0.83      0.91      0.87      1025\n           1       0.68      0.49      0.57       382\n\n    accuracy                           0.80      1407\n   macro avg       0.75      0.70      0.72      1407\nweighted avg       0.79      0.80      0.79      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "Xref",
      "code_hash": "254deb84b8b14c06df1f4bf5dfee9841",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.2s\n[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=100, penalty=l2, solver=saga; total time=   0.6s\n[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.3s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   3.5s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.9s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   2.1s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   2.7s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.4s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.8s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.9s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.4s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.8s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.6s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   2.0s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.3s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   8.7s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   2.9s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   2.0s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   5.5s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.7s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.2s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   5.7s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   8.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.7s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.6; total time=   4.7s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.2s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   2.3s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.2s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=2, n_estimators=400, subsample=1.0; total time=  11.6s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   6.5s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.8s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   1.1s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.3s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   9.4s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   7.9s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   5.3s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   4.7s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.6s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   3.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=   0.8s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.5s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=   0.5s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=1.0; total time=   0.6s\n[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.1s\n[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=100, penalty=l2, solver=saga; total time=   0.4s\n[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.9s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.3s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.3s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.8s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   2.0s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.3s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   2.5s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.4s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.2s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.2s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.6s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.4s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   3.0s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   1.9s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   2.1s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.3s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   1.8s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   4.1s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=400, subsample=0.6; total time=   4.3s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   2.9s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=400, subsample=0.8; total time=  12.7s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   2.9s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   5.2s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=500, subsample=0.8; total time=  12.5s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   2.2s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   2.2s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.3s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=2, n_estimators=400, subsample=1.0; total time=  11.2s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   6.7s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   8.2s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   0.7s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   0.8s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   4.3s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   4.1s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   3.6s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   2.4s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   6.0s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   4.7s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   1.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   0.8s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=9, n_estimators=400, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=   1.0s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=400, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=   0.4s[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=100, penalty=l2, solver=saga; total time=   0.4s\n[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.7s\n[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.1s\n[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.1s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   3.6s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.6s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   2.1s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.3s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   2.4s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.9s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.9s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   3.3s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.2s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   2.3s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   2.1s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   1.8s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   4.3s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=400, subsample=0.6; total time=   4.2s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   1.3s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   2.8s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=400, subsample=0.8; total time=  11.8s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   2.4s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   8.3s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.6s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.6; total time=   5.1s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   2.6s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   2.3s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.2s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=2, n_estimators=400, subsample=1.0; total time=  12.0s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   6.8s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   7.9s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   0.7s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   4.0s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   3.9s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   3.6s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   7.6s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   5.4s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   4.8s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.8s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   3.6s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=   0.8s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=   0.9s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=   0.6s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=1.0; total time=   0.6s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.8; total time=   0.6s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=9, n_estimators=500, subsample=0.8; total time=   0.9s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.1s\n[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.0s\n[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   1.0s\n[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.0s\n[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.0s\n[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   0.0s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   3.7s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.0s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.3s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.3s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   2.0s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.6s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   1.6s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.9s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   3.3s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   1.8s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   2.0s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   1.9s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   3.9s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=400, subsample=0.6; total time=   3.8s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   1.3s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   2.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   1.9s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   5.4s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.2s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   6.0s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   5.0s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=500, subsample=0.8; total time=  12.8s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.3s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.8s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=5, n_estimators=500, subsample=0.8; total time=   8.6s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.4s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   8.8s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=5, n_estimators=500, subsample=1.0; total time=   4.5s\n[CV] END learning_rate=0.2, max_depth=3, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   3.9s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   2.5s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   6.5s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   9.6s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   1.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   0.7s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=9, n_estimators=400, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=   0.5s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=1.0; total time=   0.7s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=9, n_estimators=500, subsample=0.8; total time=   0.9s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.8; total time=   0.4s[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.0s\n[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.0s\n[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   0.0s\n[CV] END .....................C=100, penalty=l2, solver=saga; total time=   0.5s\n[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.5s\n[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.3s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.3s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.5s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   3.5s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.3s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.8s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.9s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.2s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.7s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   3.3s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   3.5s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=   2.2s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.5s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   1.2s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   9.1s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.2s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   2.1s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   5.4s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.5s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   6.0s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   4.7s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=500, subsample=0.8; total time=  12.0s\n[CV] END learning_rate=0.05, max_depth=5, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   2.4s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.3s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=5, n_estimators=500, subsample=0.8; total time=   8.0s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   8.5s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   0.6s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.5s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.9s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   1.2s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.5s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   9.7s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=10, n_estimators=500, subsample=0.8; total time=   8.0s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   5.5s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   9.5s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   1.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=400, subsample=0.8; total time=   0.9s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8; total time=   0.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=9, n_estimators=500, subsample=0.8; total time=   1.0s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.2s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=400, subsample=1.0; total time=   0.7s\n[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.8; total time=   0.9s[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.1s\n[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   0.0s\n[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.0s\n[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   0.1s\n[CV] END .....................C=100, penalty=l2, solver=saga; total time=   0.5s\n[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.8s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.4s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   1.3s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.0s\n[CV] END bootstrap=False, max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.2s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.9s\n[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.9s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   1.0s\n[CV] END bootstrap=True, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   2.7s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=   1.4s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   1.1s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.5s\n[CV] END bootstrap=False, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   2.6s\n[CV] END bootstrap=False, max_depth=50, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   3.0s\n[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   1.2s\n[CV] END bootstrap=True, max_depth=50, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=   1.8s\n[CV] END bootstrap=False, max_depth=40, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   2.5s\n[CV] END bootstrap=True, max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time=   2.1s\n[CV] END bootstrap=False, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   1.9s\n[CV] END bootstrap=True, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.5s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=400, subsample=0.8; total time=   9.4s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   3.1s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=400, subsample=0.8; total time=  12.4s\n[CV] END learning_rate=0.01, max_depth=3, min_samples_split=10, n_estimators=400, subsample=0.6; total time=   2.6s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=2, n_estimators=400, subsample=1.0; total time=   9.1s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.4s\n[CV] END learning_rate=0.2, max_depth=5, min_samples_split=10, n_estimators=500, subsample=0.6; total time=   4.6s\n[CV] END learning_rate=0.05, max_depth=3, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   1.2s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   5.4s\n[CV] END learning_rate=0.1, max_depth=7, min_samples_split=5, n_estimators=500, subsample=0.8; total time=   8.2s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   8.6s\n[CV] END learning_rate=0.01, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.1, max_depth=5, min_samples_split=2, n_estimators=500, subsample=0.6; total time=   4.8s\n[CV] END learning_rate=0.1, max_depth=3, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   1.3s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.8s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=400, subsample=1.0; total time=   9.8s\n[CV] END learning_rate=0.05, max_depth=7, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.4s\n[CV] END learning_rate=0.01, max_depth=9, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.7s\n[CV] END learning_rate=0.01, max_depth=5, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   2.2s\n[CV] END learning_rate=0.2, max_depth=9, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   6.1s\n[CV] END learning_rate=0.05, max_depth=9, min_samples_split=5, n_estimators=500, subsample=0.6; total time=   9.0s\n[CV] END learning_rate=0.2, max_depth=7, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   3.6s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=1.0; total time=   0.7s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=9, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=300, subsample=0.8; total time=   1.0s\n[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=400, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.8; total time=   0.6s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.3s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=400, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=   0.4s\n[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.8; total time=   0.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.2s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=400, subsample=1.0; total time=   0.8s\n[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.8; total time=   1.0s\n[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=9, n_estimators=300, subsample=0.8; total time=   0.7sROC AUC: 0.8443161792874473\nAccuracy: 0.7995735607675906\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.87      1025\n           1       0.67      0.52      0.58       382\n\n    accuracy                           0.80      1407\n   macro avg       0.75      0.71      0.73      1407\nweighted avg       0.79      0.80      0.79      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "SFPL",
      "code_hash": "f1db743df161b23f6162028b667e7cf8",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.7938877043354655\nTest ROC AUC: 0.8483054526880348\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.86      1025\n           1       0.66      0.50      0.57       382\n\n    accuracy                           0.79      1407\n   macro avg       0.74      0.70      0.72      1407\nweighted avg       0.78      0.79      0.78      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "BYtC",
      "code_hash": "eadd98f0791998a5eb8aaffce79a0eb2",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n/home/lasse/Documents/aml_exam/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n  warnings.warn(\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Test Accuracy: 0.7981520966595593\nTest ROC AUC: 0.848586387434555\n              precision    recall  f1-score   support\n\n           0       0.84      0.90      0.87      1025\n           1       0.66      0.53      0.59       382\n\n    accuracy                           0.80      1407\n   macro avg       0.75      0.71      0.73      1407\nweighted avg       0.79      0.80      0.79      1407\n\n",
          "mimetype": "text/plain"
        }
      ]
    }
  ]
}