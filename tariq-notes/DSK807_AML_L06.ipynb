{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5rG1V7Sr30o"
      },
      "source": [
        "## Essential code to build neural networks using TensorFlow\n",
        "\n",
        "1. To initialize a simple sequential model we use $\\texttt{tf.keras.models.Sequential}$.\n",
        "1. To implement a simple linear transformation with some activation we use $\\texttt{tf.keras.models.layers.Dense}$.\n",
        "1. After building a model, we need to **compile** it, where we use the method $\\texttt{.compile}$. This is essentially to tell that we are \"finished\" with initializing our model, so it may be considered a part of the initialization process.\n",
        "1. Training is exactly like with $\\texttt{sklearn}$, i.e. we use the method $\\texttt{.fit}$.\n",
        "1. We can also predict much like as with $\\texttt{sklearn}$, i.e. using the method $\\texttt{.predict}$. However, to evaluate our model, it is easier to use the method $\\texttt{.evaluate}$.\n",
        "1. We can take a look at our model using the method $\\texttt{.summary}$, which is super cool and useful!\n",
        "\n",
        "**Note**: There are more advanced (but also more flexible) ways to initialize and train a model. However, the above \"steps\" are useful to solve most problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer Wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 569\n",
            "\n",
            ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            ":Attribute Information:\n",
            "    - radius (mean of distances from center to points on the perimeter)\n",
            "    - texture (standard deviation of gray-scale values)\n",
            "    - perimeter\n",
            "    - area\n",
            "    - smoothness (local variation in radius lengths)\n",
            "    - compactness (perimeter^2 / area - 1.0)\n",
            "    - concavity (severity of concave portions of the contour)\n",
            "    - concave points (number of concave portions of the contour)\n",
            "    - symmetry\n",
            "    - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "    worst/largest values) of these features were computed for each image,\n",
            "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
            "    10 is Radius SE, field 20 is Worst Radius.\n",
            "\n",
            "    - class:\n",
            "            - WDBC-Malignant\n",
            "            - WDBC-Benign\n",
            "\n",
            ":Summary Statistics:\n",
            "\n",
            "===================================== ====== ======\n",
            "                                        Min    Max\n",
            "===================================== ====== ======\n",
            "radius (mean):                        6.981  28.11\n",
            "texture (mean):                       9.71   39.28\n",
            "perimeter (mean):                     43.79  188.5\n",
            "area (mean):                          143.5  2501.0\n",
            "smoothness (mean):                    0.053  0.163\n",
            "compactness (mean):                   0.019  0.345\n",
            "concavity (mean):                     0.0    0.427\n",
            "concave points (mean):                0.0    0.201\n",
            "symmetry (mean):                      0.106  0.304\n",
            "fractal dimension (mean):             0.05   0.097\n",
            "radius (standard error):              0.112  2.873\n",
            "texture (standard error):             0.36   4.885\n",
            "perimeter (standard error):           0.757  21.98\n",
            "area (standard error):                6.802  542.2\n",
            "smoothness (standard error):          0.002  0.031\n",
            "compactness (standard error):         0.002  0.135\n",
            "concavity (standard error):           0.0    0.396\n",
            "concave points (standard error):      0.0    0.053\n",
            "symmetry (standard error):            0.008  0.079\n",
            "fractal dimension (standard error):   0.001  0.03\n",
            "radius (worst):                       7.93   36.04\n",
            "texture (worst):                      12.02  49.54\n",
            "perimeter (worst):                    50.41  251.2\n",
            "area (worst):                         185.2  4254.0\n",
            "smoothness (worst):                   0.071  0.223\n",
            "compactness (worst):                  0.027  1.058\n",
            "concavity (worst):                    0.0    1.252\n",
            "concave points (worst):               0.0    0.291\n",
            "symmetry (worst):                     0.156  0.664\n",
            "fractal dimension (worst):            0.055  0.208\n",
            "===================================== ====== ======\n",
            "\n",
            ":Missing Attribute Values: None\n",
            "\n",
            ":Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            ":Donor: Nick Street\n",
            "\n",
            ":Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. dropdown:: References\n",
            "\n",
            "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
            "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
            "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "    San Jose, CA, 1993.\n",
            "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
            "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
            "    July-August 1995.\n",
            "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
            "    163-171.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "print(load_breast_cancer().DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SsBLl7Xcrgyw"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#@title Breast Cancer Classification with NN\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use the `load_breast_cancer` function to construct your dataset\n",
        "x, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Use `train_test_split` to split your data into a train and a test set.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "z_train = scaler.fit_transform(x_train)\n",
        "z_test = scaler.transform(x_test)\n",
        "\n",
        "our_first_nn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(z_train.shape[1],)),\n",
        "    # input shape required in the first layer\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    # map 64 to 128 features and apply ReLU\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # softmax is used for classification\n",
        "    tf.keras.layers.Dense(2, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "our_first_nn.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KsOLse7sEuT"
      },
      "outputs": [],
      "source": [
        "our_first_nn.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "our_first_nn.fit(z_train, y_train, epochs=3)\n",
        "\n",
        "loss, accuracy = our_first_nn.evaluate(z_test, y_test)\n",
        "\n",
        "print(\n",
        "    f'\\nOut first neural network managed {round(accuracy * 100, 2)}% accuracy.'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQty2-2OsHQO"
      },
      "outputs": [],
      "source": [
        "#@title Classification Example (MNIST Digits)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize (scale 0-255 pixel values to 0-1)\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Flatten images (28x28 → 784)\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(784,)),      # Explicit input layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afErxIsLvASd"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1knpVUVhvgHR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assume `model` is your Keras model\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file=\"model.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2YVgSjrsusU"
      },
      "outputs": [],
      "source": [
        "#@title Regression\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdNoisztuJLC"
      },
      "outputs": [],
      "source": [
        "# Build model with Input layer\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),  # Explicit input layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=['mae'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Test MAE: {test_mae:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lbZv81nv6Lk"
      },
      "outputs": [],
      "source": [
        "# Assume `model` is your Keras model\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file=\"model.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM2uvwrmuY44"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation MAE\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation MAE\")\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss (MSE)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFpq33jIDTX8"
      },
      "source": [
        "## **Task 1**: Binary Classification with SVM (Banknote Authentication)\n",
        "![](https://www.neuraldesigner.com/images/banknote-authentication.webp)\n",
        "[Image Source](https://www.neuraldesigner.com/blog/banknote-authentication/)\n",
        "\n",
        "- **Dataset**: Banknote Authentication — wavelet-transformed images of banknotes.\n",
        "- **Target**: Predict whether a banknote is `genuine` (1) or `fake` (0).\n",
        "\n",
        "**Use the file \"banknotes.csv\"**\n",
        "\n",
        "- Train a classification model using NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WqftWIVvJcS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
