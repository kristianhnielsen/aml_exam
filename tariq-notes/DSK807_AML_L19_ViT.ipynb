{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVIB6Y4geuRZ",
        "outputId": "c230acf9-a373-4c20-e78b-18f11645f421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/2.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ2H1v-7TiUM"
      },
      "source": [
        "# Visual Transformers (ViT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "a055f3e01303413f849e4fcd09652e5d",
            "db9b6a275f5d417ba8a273a40a508191",
            "d6a90a8c94bd4cdc9b27ceb8bc5e66fe"
          ]
        },
        "id": "IrGlmvCkRQnj",
        "outputId": "2499aafd-9b56-4447-c6dd-196d668ec2e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a055f3e01303413f849e4fcd09652e5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db9b6a275f5d417ba8a273a40a508191",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6a90a8c94bd4cdc9b27ceb8bc5e66fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oefFGGgLRRdb",
        "outputId": "9046e448-39f9-4720-aea1-d7f26f92ffdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: Egyptian cat\n"
          ]
        }
      ],
      "source": [
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "# model predicts one of the 1000 ImageNet classes\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XvrTsiOBVPCO"
      },
      "outputs": [],
      "source": [
        "# Examples of pretrained models\n",
        "# https://huggingface.co/abhilash88/face-emotion-detection\n",
        "# https://huggingface.co/AMfeta99/vit-base-oxford-brain-tumor_x-ray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXQpnnGT09ag"
      },
      "source": [
        "# Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGdgzvO_Wj40"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tFpzd5JfPUk"
      },
      "outputs": [],
      "source": [
        "example = \"en-AU-NatashaNeural.mp3\"\n",
        "\n",
        "result = pipe(example, return_timestamps=True)\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf4EE3upgkly"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "import base64\n",
        "\n",
        "def record(sec=10, filename=\"myvoice.wav\"):\n",
        "    display(Javascript(\"\"\"\n",
        "        async function record(sec){\n",
        "          const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "          const recorder = new MediaRecorder(stream);\n",
        "          let data = [];\n",
        "\n",
        "          recorder.ondataavailable = event => data.push(event.data);\n",
        "          alert(\"Start recording ...\")\n",
        "          recorder.start();\n",
        "\n",
        "          await sleep(sec * 1000);\n",
        "          recorder.stop();\n",
        "          alert(\"Done .. Saving in progress ...\")\n",
        "          await new Promise(resolve => recorder.onstop = resolve);\n",
        "\n",
        "          const blob = new Blob(data);\n",
        "          const reader = new FileReader();\n",
        "          reader.readAsDataURL(blob);\n",
        "\n",
        "          reader.onloadend = function() {\n",
        "            google.colab.kernel.invokeFunction('notebook.receive_audio', [reader.result], {});\n",
        "          };\n",
        "        }\n",
        "        record(%d);\n",
        "    \"\"\" % sec))\n",
        "\n",
        "    # callback in Python\n",
        "    def _callback(audio_data):\n",
        "        audio_bytes = base64.b64decode(audio_data.split(',')[1])\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(audio_bytes)\n",
        "        print(\"Audio saved to\", filename)\n",
        "\n",
        "    output.register_callback('notebook.receive_audio', _callback)\n",
        "\n",
        "print(\"Recorder ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Awkuu46knM_"
      },
      "outputs": [],
      "source": [
        "record()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGsk9LTAk7IP"
      },
      "outputs": [],
      "source": [
        "result = pipe(\"myvoice.wav\", return_timestamps=True)\n",
        "result[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQFdqR7k26hO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0J16v526l5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUmLarq26u4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iiBAUJG261p"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqLjUuAK264_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmD7FAwXu-ZU"
      },
      "source": [
        "# CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6sqAuPNlcLw"
      },
      "outputs": [],
      "source": [
        "clip = pipeline(\n",
        "   task=\"zero-shot-image-classification\",\n",
        "   model=\"openai/clip-vit-base-patch32\",\n",
        "   dtype=torch.bfloat16,\n",
        "   device=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDP1_QMEu_kI"
      },
      "outputs": [],
      "source": [
        "labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a dog and a cat\", \"a photo of a car\"]\n",
        "clip(\"http://images.cocodataset.org/val2017/000000039769.jpg\", candidate_labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTkHrrgwUsm"
      },
      "outputs": [],
      "source": [
        "clip(\"car.jpg\", candidate_labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcOjbilOxyh9"
      },
      "outputs": [],
      "source": [
        "clip(\"Dog_Breeds.jpg\", candidate_labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G74T4VowZZu"
      },
      "outputs": [],
      "source": [
        "clip(\"dog_and_cat.jpg\", candidate_labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXxbDY-CxKmF"
      },
      "outputs": [],
      "source": [
        "plate_labels = [\"text on the plate is: HH CT 3874\",\n",
        "                \"text on the plate is: HH CT 3674\",\n",
        "                \"text on the plate is: L QF 2255\",\n",
        "                \"text on the plate is: B NE 0001\",]\n",
        "clip(\"plate.jpg\", candidate_labels=plate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCrZWNU6zxJx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}