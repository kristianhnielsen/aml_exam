{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & setup (run this first)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "Kw_tFSHpGrJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Classification Trees"
      ],
      "metadata": {
        "id": "z8HWL90LTbms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example**: Binary Classification with DT (Banknote Authentication)\n",
        "\n",
        "<img src=\"https://www.neuraldesigner.com/images/banknote-authentication.webp\" height=200>\n",
        "\n",
        "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0925231213003202-gr6.jpg\" height=200>\n",
        "\n",
        "[Image 1 Source](https://www.neuraldesigner.com/blog/banknote-authentication/),\n",
        "[Image 2 Source](https://www.sciencedirect.com/science/article/abs/pii/S0925231213003202)"
      ],
      "metadata": {
        "id": "aQi6PP7_wNld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1) Load the dataset**\n",
        "\n",
        "bank_data = pd.read_csv('banknotes.csv')\n",
        "bank_data.head()"
      ],
      "metadata": {
        "id": "duPoP7_uxCV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare predictors and target\n",
        "X = bank_data.drop('class', axis=1)\n",
        "y = bank_data['class']\n",
        "\n",
        "# Peek at the data\n",
        "print(\"Shape:\", X.shape)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "CefP4800xjib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2) Split the dataset into train/test datasets**\n",
        "\n",
        "# Keep the test set aside for **final** evaluation.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "4VZls_a2xyw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3) Train a Baseline Decision Tree (default settings)**\n",
        "\n",
        "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
        "dt_baseline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_test = dt_baseline.predict(X_test)\n",
        "\n",
        "# Get accuracy score\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Baseline Test Accuracy: {acc_test:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report (test):\\n\")\n",
        "print(classification_report(y_test, y_pred_test, target_names=[\"Fake\",\"Genuine\"]))"
      ],
      "metadata": {
        "id": "_IhxF2qqx7eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt=\"\")\n"
      ],
      "metadata": {
        "id": "4wxEfGZZyPbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **4) Print the Decision Tree**\n",
        "fig = plt.figure(figsize=plt.figaspect(0.35))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "plot_tree(dt_baseline,\n",
        "          filled=True,\n",
        "          class_names=[\"Fake\",\"Genuine\"],\n",
        "          feature_names=X.columns,\n",
        "          ax=ax,\n",
        "          fontsize=6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vCXZfZc7zW7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **5) Feature importance**\n",
        "\n",
        "importances = pd.Series(dt_baseline.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "importances\n"
      ],
      "metadata": {
        "id": "GnGRIqEC9PK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyV5yekwmXOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Example 2** - Cancer Classification Using Decision Trees\n",
        "\n",
        "Let's try to use DTs to diagnose (breast) cancer patients. Further, let us investigate how various settings impact the performance.\n",
        "\n",
        "- Load and inspect a real medical dataset.\n",
        "- Build a baseline Decision Tree classifier (with default settings).\n",
        "- Evaluate performance (accuracy, confusion matrix, precision/recall/F1).\n",
        "- Explore how different hyperparameters affect performance:\n",
        "  - `max_depth` (integer or `None`)\n",
        "  - `min_samples_split` (integer or fraction)\n",
        "  - `min_samples_leaf` (integer or fraction)\n",
        "  - `max_features` (integer, fraction, `\"sqrt\"`, `\"log2\"`, or `None`)  \n",
        "\n",
        "![What does breast cancer look like on mammography](https://healthimaging.com/sites/default/files/styles/gallery/public/2022-09/Series%20on%20annual%20mammograms%20showing%20cancer%20formation_RSNA.jpg.webp?itok=7rAqFvmS)\n",
        "\n",
        "[image source](https://healthimaging.com/topics/medical-imaging/womens-imaging/breast-imaging/photo-gallery-what-does-breast-cancer-look-mammography)"
      ],
      "metadata": {
        "id": "vqy1OaLFD5Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  1) Load & explore the dataset\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "\n",
        "# Peek at the data\n",
        "print(\"Shape:\", X.shape)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "n42gqDtpGxov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "y.value_counts(normalize=True).rename(index={0:\"malignant\", 1:\"benign\"})"
      ],
      "metadata": {
        "id": "V-vI7ACGHAUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Create a train/test split\n",
        "\n",
        "# Keep the test set aside for **final** evaluation.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "--5htqjRHJEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Baseline Decision Tree (default settings)\n",
        "\n",
        "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
        "dt_baseline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_test = dt_baseline.predict(X_test)\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"Baseline Test Accuracy: {acc_test:.4f}\")\n",
        "print(\"\\nClassification report (test):\\n\")\n",
        "print(classification_report(y_test, y_pred_test, target_names=[\"malignant\",\"benign\"]))"
      ],
      "metadata": {
        "id": "YwnQZFYOHkV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "# Plot confusion matrix\n",
        "classes = [\"malignant\",\"benign\"]\n",
        "sns.heatmap(cm, annot=True, xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "# Set the axis labels and title\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3m17QstEH1xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth_values = [None] + list(range(1, 21))  # None means expand until all leaves are pure or min_samples constraints are met\n",
        "cv_means = []\n",
        "\n",
        "for d in depth_values:\n",
        "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "    cv_means.append(scores.mean())\n",
        "\n",
        "# Plot\n",
        "plt.figure()\n",
        "plt.plot([str(d) for d in depth_values], cv_means, marker=\"o\")\n",
        "plt.title(\"Cross-validated Accuracy vs max_depth\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_idx = int(np.argmax(cv_means))\n",
        "best_depth = depth_values[best_idx]\n",
        "print(\"Best max_depth by CV:\", best_depth, \"with mean accuracy:\", f\"{cv_means[best_idx]:.4f}\")"
      ],
      "metadata": {
        "id": "Y0KWpMvvIOXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_values = [2, 5, 10, 20, 0.01, 0.05, 0.1, 0.2]\n",
        "cv_means_split = []\n",
        "\n",
        "for s in split_values:\n",
        "    clf = DecisionTreeClassifier(min_samples_split=s, random_state=42)\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "    cv_means_split.append(scores.mean())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([str(s) for s in split_values], cv_means_split, marker=\"o\")\n",
        "plt.title(\"Cross-validated Accuracy vs min_samples_split\")\n",
        "plt.xlabel(\"min_samples_split\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_idx = int(np.argmax(cv_means_split))\n",
        "print(\"Best min_samples_split by CV:\", split_values[best_idx], \"with mean accuracy:\", f\"{cv_means_split[best_idx]:.4f}\")"
      ],
      "metadata": {
        "id": "49vIon0_lFYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "leaf_values = [1, 2, 5, 10, 20, 0.01, 0.02, 0.05, 0.1]\n",
        "cv_means_leaf = []\n",
        "\n",
        "for l in leaf_values:\n",
        "    clf = DecisionTreeClassifier(min_samples_leaf=l, random_state=42)\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "    cv_means_leaf.append(scores.mean())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([str(l) for l in leaf_values], cv_means_leaf, marker=\"o\")\n",
        "plt.title(\"Cross-validated Accuracy vs min_samples_leaf\")\n",
        "plt.xlabel(\"min_samples_leaf\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_idx = int(np.argmax(cv_means_leaf))\n",
        "print(\"Best min_samples_leaf by CV:\", leaf_values[best_idx], \"with mean accuracy:\", f\"{cv_means_leaf[best_idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "M_IxdiIhlXsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_settings = [None, \"sqrt\", \"log2\", 5, 10, 0.5, 0.8]\n",
        "cv_means_feats = []\n",
        "\n",
        "for f in feature_settings:\n",
        "    clf = DecisionTreeClassifier(max_features=f, random_state=42)\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
        "    cv_means_feats.append(scores.mean())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([str(f) for f in feature_settings], cv_means_feats, marker=\"o\")\n",
        "plt.title(\"Cross-validated Accuracy vs max_features\")\n",
        "plt.xlabel(\"max_features\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_idx = int(np.argmax(cv_means_feats))\n",
        "print(\"Best max_features by CV:\", feature_settings[best_idx], \"with mean accuracy:\", f\"{cv_means_feats[best_idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "B6SfbHkqlh84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tree_para = {\n",
        "    'criterion':['gini','entropy'],\n",
        "    'max_depth': list(range(3, 30)),\n",
        "    'min_samples_leaf': [1, 2, 5, 10, 20, 0.01, 0.02, 0.05, 0.1],\n",
        "    'max_features': [None, \"sqrt\", \"log2\", 5, 10, 0.5, 0.8],\n",
        "    'min_samples_split': [2, 5, 10, 20, 0.01, 0.05, 0.1, 0.2]\n",
        "    }\n",
        "# clf = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
        "#                    param_grid=tree_para,\n",
        "#                    cv=5,\n",
        "#                    scoring='accuracy')\n",
        "# clf.fit(X_train, y_train)\n",
        "# print(clf.best_params_)"
      ],
      "metadata": {
        "id": "KkIucoECp5Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ‘‰ TODO: Replace these with the values you consider best from the sweeps above\n",
        "best_params = {'criterion': 'entropy', 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
        "\n",
        "dt_final = DecisionTreeClassifier(random_state=42, **best_params)\n",
        "dt_final.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test_final = dt_final.predict(X_test)\n",
        "acc_test_final = accuracy_score(y_test, y_pred_test_final)\n",
        "print(\"Final Test Accuracy:\", f\"{acc_test_final:.4f}\")\n",
        "print(\"\\nClassification report (test):\\n\")\n",
        "print(classification_report(y_test, y_pred_test_final, target_names=[\"malignant\",\"benign\"]))\n",
        "\n",
        "# Confusion Matrix for final model\n",
        "cm_final = confusion_matrix(y_test, y_pred_test_final)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(cm_final, interpolation='nearest')\n",
        "plt.title(\"Final Model: Confusion Matrix (Test)\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0,1], [\"malignant\",\"benign\"])\n",
        "plt.yticks([0,1], [\"malignant\",\"benign\"])\n",
        "for i in range(cm_final.shape[0]):\n",
        "    for j in range(cm_final.shape[1]):\n",
        "        plt.text(j, i, cm_final[i, j], ha=\"center\", va=\"center\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K22nyFKTlqQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = pd.Series(dt_final.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 10))\n",
        "importances.iloc[:20].plot(kind=\"barh\")\n",
        "plt.title(\"Top 20 Feature Importances\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "importances.head(10)\n"
      ],
      "metadata": {
        "id": "b1oGQBaMl-6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=plt.figaspect(0.35))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "plot_tree(dt_final, filled=True, class_names=[\"malignant\",\"benign\"],\n",
        "               feature_names=data.feature_names, ax=ax, fontsize=6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iejreikpmx2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = dt_final.feature_importances_\n",
        "names = load_breast_cancer()['feature_names']\n",
        "\n",
        "feature_importance = pd.DataFrame(zip(names, importances),\n",
        "                                  columns=['Feature', 'Importance'])\n",
        "feature_importance = feature_importance.sort_values(\n",
        "    'Importance', ascending=False).reset_index()\n",
        "feature_importance[:10]"
      ],
      "metadata": {
        "id": "eQvqjhXjqCMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "When used for classification, the trees \"vote\" when predicting. Use $\\texttt{RandomForestClassifier}$. [Classifier](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.ensemble.RandomForestClassifier.html#:~:text=class%20sklearn.ensemble.)\n",
        "\n",
        "When used for regression, the mean of the individual trees' predictions are used. Use $\\texttt{RandomForestRegressor}$. [Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
        "\n",
        "The parameters of the random forest are almost like those of a decision tree (after all, it is just multiple decision trees). The main new things are:\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. An integer. Any sufficiently large value is good.\n",
        "1. **max_features**: The number of features to consider when looking for the best split."
      ],
      "metadata": {
        "id": "NR1PuAF9nXbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble\n",
        "\n",
        "# Initialize\n",
        "rf = ensemble.RandomForestClassifier()\n",
        "\n",
        "# Fit\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_test_hat = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_test_hat)\n",
        "print(\n",
        "    f'''RF with default settings achieved {round(accuracy * 100, 1)}% accuracy.'''\n",
        ")"
      ],
      "metadata": {
        "id": "OWLht9denUTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# base models (note: scale LR/SVM but not DT)\n",
        "lr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=200, class_weight=\"balanced\"))\n",
        "])\n",
        "svm = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", SVC(kernel=\"rbf\", C=1.0, probability=True, class_weight=\"balanced\"))\n",
        "])\n",
        "dt = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight=\"balanced\")\n",
        "\n",
        "# Soft voting (recommended when proba available)\n",
        "soft_voter = VotingClassifier(\n",
        "    estimators=[(\"lr\", lr), (\"svm\", svm), (\"dt\", dt)],\n",
        "    voting=\"soft\",  # average probabilities\n",
        "    weights=[1, 1, 1]  # you can tune these\n",
        ")\n",
        "\n",
        "soft_voter.fit(X_train, y_train)\n",
        "y_pred = soft_voter.predict(X_test)\n",
        "print(\"Soft Voting - Acc:\", accuracy_score(y_test, y_pred), \"F1:\", f1_score(y_test, y_pred))\n",
        "\n",
        "# Hard voting (majority vote on class labels)\n",
        "hard_voter = VotingClassifier(\n",
        "    estimators=[(\"lr\", lr), (\"svm\", svm), (\"dt\", dt)],\n",
        "    voting=\"hard\"\n",
        ")\n",
        "hard_voter.fit(X_train, y_train)\n",
        "y_pred = hard_voter.predict(X_test)\n",
        "print(\"Hard Voting - Acc:\", accuracy_score(y_test, y_pred), \"F1:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qZMf_FIv8J9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Task"
      ],
      "metadata": {
        "id": "nJos8T7uopUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'HousingData.csv'\n",
        "raw_df = pd.read_csv(data).dropna()\n",
        "\n",
        "# Create a copy of the DataFrame with column names\n",
        "df_copy = raw_df.copy()\n",
        "\n",
        "# Separate the target variable (y) and features (X)\n",
        "y = df_copy['MEDV']  # Replace 'TargetColumn' with your actual target column name\n",
        "X = df_copy.drop(columns=['MEDV'])  # Remove the target column\n",
        "\n",
        "# We use `train_test_split` to split our data into a train and a test set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "xS7IlLyNnm-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_base = DecisionTreeRegressor(random_state=42)\n",
        "dt_base.fit(X_train, y_train)\n",
        "\n",
        "y_te_pred = dt_base.predict(X_test)\n",
        "\n",
        "print(\"Baseline Decision Tree Regressor\")\n",
        "print(\"-\"*32)\n",
        "print(\"Test MSE: %.3f\" % ( mean_squared_error(y_test, y_te_pred)))"
      ],
      "metadata": {
        "id": "owmJLxO-oyRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "rf = ensemble.RandomForestRegressor()\n",
        "\n",
        "# Fit\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_test_hat = rf.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_test_hat)\n",
        "print(f'''RF with default settings achieved {round(mse, 3)} MSE.''')"
      ],
      "metadata": {
        "id": "jFZoH2yaogtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_settings = [None, \"sqrt\", \"log2\", 5, 10, 0.5, 0.6, 0.7, 0.8]\n",
        "estimators = [ 120, 180, 240, 300]\n",
        "\n",
        "for f in feature_settings:\n",
        "  for n in estimators:\n",
        "    # Initialize\n",
        "    rf = ensemble.RandomForestRegressor(max_features=f, n_estimators=n)\n",
        "\n",
        "    # Fit\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_test_hat = rf.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_test_hat)\n",
        "    print(f'''RF with max_features={f} and n_estimators={n} achieved {round(mse, 3)} MSE.''')"
      ],
      "metadata": {
        "id": "FuLTpJ40oj5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "t82lbaempFli"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}